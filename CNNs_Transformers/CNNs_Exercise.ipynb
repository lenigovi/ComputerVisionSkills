{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be5819e538b230ddf5f888fc4481212b",
     "grade": false,
     "grade_id": "cell-67e1fcdd86afffc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Vision Models\n",
    "\n",
    "Building on top of the previous exercise, we will now implement more sophisticated models for image classification, namely a ResNet and a Vision Transformer.\n",
    "Augmented training data and a function for training is provided, so we can focus on building these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5e7586ab7ee5df80098dce85973a26c",
     "grade": false,
     "grade_id": "cell-321ba003b1b79c95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "LOG_ROOT = 'tensorboard_logs'\n",
    "USE_GPU = True  # Set to True if you have installed tensorflow for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55ab6d56a1d5caefbec869dc6240092d",
     "grade": false,
     "grade_id": "cell-1073cd4f1b08084c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Just an image plotting function\n",
    "def plot_multiple(images, titles=None, colormap='gray',\n",
    "                  max_columns=np.inf, imwidth=2, imheight=2, share_axes=False):\n",
    "    \"\"\"\n",
    "    Plot multiple images as subplots on a grid. Images must be channel-first\n",
    "    and between [0, 1].\n",
    "    \"\"\"\n",
    "    images = [np.transpose(im, (1, 2, 0)) for im in images]\n",
    "    if titles is None:\n",
    "        titles = [''] * len(images)\n",
    "    assert len(images) == len(titles)\n",
    "    n_images = len(images)\n",
    "    n_cols = min(max_columns, n_images)\n",
    "    n_rows = int(np.ceil(n_images / n_cols))\n",
    "    fig, axes = plt.subplots(\n",
    "        n_rows, n_cols, figsize=(n_cols * imwidth, n_rows * imheight),\n",
    "        squeeze=False, sharex=share_axes, sharey=share_axes)\n",
    "\n",
    "    axes = axes.flat\n",
    "    # Hide subplots without content\n",
    "    for ax in axes[n_images:]:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    if not isinstance(colormap, (list,tuple)):\n",
    "        colormaps = [colormap]*n_images\n",
    "    else:\n",
    "        colormaps = colormap\n",
    "\n",
    "    for ax, image, title, cmap in zip(axes, images, titles, colormaps):\n",
    "        ax.imshow(image, cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    \n",
    "def visualize_dataset(dataset, n_samples=24, max_columns=6):\n",
    "    xs, ys = list(zip(*[dataset[i] for i in range(n_samples)]))\n",
    "    plot_multiple([x / 2 + 0.5 for x in xs], [labels[i] for i in ys], max_columns=max_columns)\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    opt,\n",
    "    logdir,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    n_epochs=50,\n",
    "    lr_scheduler=None,\n",
    "):\n",
    "    # stuff for writing to tensorboard\n",
    "    writer = SummaryWriter(f\"{LOG_ROOT}/{logdir}-{time.strftime('%y%m%d_%H%M%S')}\")\n",
    "    layout = {\n",
    "        'Losses': {'losses': ['Multiline', ['loss/train', 'loss/test']]},\n",
    "        'Accuracy': {'accuracy': ['Multiline', ['accuracy/train', 'accuracy/test']]}\n",
    "    }\n",
    "    writer.add_custom_scalars(layout)\n",
    "    \n",
    "    start = time.time()  # get current time for statistics\n",
    "    for epoch in range(n_epochs):  # iterate over epochs\n",
    "        model.train()  # set the model to training mode\n",
    "        # collect some numbers for statistics\n",
    "        sample_count = 0  # number of seen samples\n",
    "        loss_sum = 0  # total loss\n",
    "        correct_count = 0  # total number of correctly classified samples\n",
    "        n_batches = len(train_loader)  # number of batches\n",
    "        for i, (xs, ys) in enumerate(train_loader):  # iterate over training set\n",
    "            # training step\n",
    "            if USE_GPU:\n",
    "                xs = xs.cuda()\n",
    "                ys = ys.cuda()\n",
    "            out = model(xs)\n",
    "            loss = criterion(out, ys)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # update statistics\n",
    "            loss_sum += loss.item() * xs.shape[0]\n",
    "            _, pred = torch.max(out, 1)\n",
    "            correct_count += (pred == ys).sum().item()\n",
    "            sample_count += xs.shape[0]\n",
    "            print(f'Train epoch {epoch+1}, step {i+1}/{n_batches}', end='    \\r')\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        train_loss = loss_sum / sample_count\n",
    "        train_accuracy = correct_count / sample_count\n",
    "        \n",
    "        with torch.no_grad():  # do not store gradients during testing, decreases memory consumption\n",
    "            model.eval()  # set the model to evaluation mode\n",
    "            sample_count = 0\n",
    "            loss_sum = 0\n",
    "            correct_count = 0\n",
    "            n_batches = len(test_loader)\n",
    "            for i, (xs, ys) in enumerate(test_loader):\n",
    "                if USE_GPU:\n",
    "                    xs = xs.cuda()\n",
    "                    ys = ys.cuda()\n",
    "                out = model(xs)\n",
    "                loss = criterion(out, ys)\n",
    "                loss_sum += loss.item() * xs.shape[0]\n",
    "                _, pred = torch.max(out, 1)\n",
    "                correct_count += (pred == ys).sum().item()\n",
    "                sample_count += xs.shape[0]\n",
    "                print(f'Test epoch {epoch+1}, step {i+1}/{n_batches}', end='    \\r')\n",
    "                \n",
    "            test_loss = loss_sum / sample_count\n",
    "            test_accuracy = correct_count / sample_count\n",
    "        \n",
    "        writer.add_scalar('loss/train', train_loss, epoch+1)\n",
    "        writer.add_scalar('accuracy/train', train_accuracy, epoch+1)\n",
    "        writer.add_scalar('loss/test', test_loss, epoch+1)\n",
    "        writer.add_scalar('accuracy/test', test_accuracy, epoch+1)\n",
    "            \n",
    "        print(\n",
    "            f'Epoch {epoch+1} | train loss: {train_loss:.3f}, train accuracy: {train_accuracy:.3f}, ' + \\\n",
    "            f'test loss: {test_loss:.3f}, test accuracy: {test_accuracy:.3f}, ' + \\\n",
    "            f'time: {str(datetime.timedelta(seconds=int(time.time()-start)))}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f3c0d5326fb908b1378416f92459cbe",
     "grade": false,
     "grade_id": "cell-31d81f8c56fa930d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalize_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.RandomHorizontalFlip()\n",
    "])\n",
    "\n",
    "train_data = CIFAR10(root='cifar10/train/', train=True, download=True, transform=transforms.Compose([augment_transform, normalize_transform]))\n",
    "test_data = CIFAR10(root='cifar10/test/', train=False, download=True, transform=normalize_transform)\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=6)\n",
    "test_loader = DataLoader(test_data, batch_size=128, num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8caae469ea7013cff26fa649c7d24662",
     "grade": false,
     "grade_id": "cell-87f2ee7421f4b385",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Simple CNN\n",
    "\n",
    "As a starting point we use the CNN you already know from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2f2e18af80a0d6ba370410219229ebd",
     "grade": false,
     "grade_id": "cell-71ed3ce0361a6f09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/25\n",
      "Finished epoch 1/25\n",
      "Starting epoch 2/25\n",
      "Finished epoch 2/25\n",
      "Starting epoch 3/25\n",
      "Finished epoch 3/25\n",
      "Starting epoch 4/25\n",
      "Finished epoch 4/25\n",
      "Starting epoch 5/25\n",
      "Finished epoch 5/25\n",
      "Starting epoch 6/25\n",
      "Finished epoch 6/25\n",
      "Starting epoch 7/25\n",
      "Finished epoch 7/25\n",
      "Starting epoch 8/25\n",
      "Finished epoch 8/25\n",
      "Starting epoch 9/25\n",
      "Finished epoch 9/25\n",
      "Starting epoch 10/25\n",
      "Finished epoch 10/25\n",
      "Starting epoch 11/25\n",
      "Finished epoch 11/25\n",
      "Starting epoch 12/25\n",
      "Finished epoch 12/25\n",
      "Starting epoch 13/25\n",
      "Finished epoch 13/25\n",
      "Starting epoch 14/25\n",
      "Finished epoch 14/25\n",
      "Starting epoch 15/25\n",
      "Finished epoch 15/25\n",
      "Starting epoch 16/25\n",
      "Finished epoch 16/25\n",
      "Starting epoch 17/25\n",
      "Finished epoch 17/25\n",
      "Starting epoch 18/25\n",
      "Finished epoch 18/25\n",
      "Starting epoch 19/25\n",
      "Finished epoch 19/25\n",
      "Starting epoch 20/25\n",
      "Finished epoch 20/25\n",
      "Starting epoch 21/25\n",
      "Finished epoch 21/25\n",
      "Starting epoch 22/25\n",
      "Finished epoch 22/25\n",
      "Starting epoch 23/25\n",
      "Finished epoch 23/25\n",
      "Starting epoch 24/25\n",
      "Finished epoch 24/25\n",
      "Starting epoch 25/25\n",
      "Finished epoch 25/25\n"
     ]
    }
   ],
   "source": [
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=4096, out_features=10)\n",
    ")\n",
    "if USE_GPU:\n",
    "    cnn.cuda()\n",
    "\n",
    "train_model(\n",
    "    model=cnn,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    opt=optim.Adam(cnn.parameters(), lr=5e-4),\n",
    "    logdir='cnn',\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4241ec86a3be34fad9c89a527182780",
     "grade": false,
     "grade_id": "cell-959a911a2a175121",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Batch normalization is a technique to improve and speed up the training of deep neural networks. Each feature channel is normalized to have zero mean and unit variance across the spatial and mini-batch axes. To compensate for the lost degrees of freedom, extra scaling and bias parameters are introduced and learned. Mathematically, BatchNorm for a spatial feature map (e.g. the output of conv) can be written as:\n",
    "\n",
    "$$\n",
    "\\mu_d = \\mathbb{E}\\{x_{\\cdot \\cdot d}\\}, \\\\\n",
    "\\sigma_d = \\sqrt{\\operatorname{Var}\\{x_{\\cdot \\cdot d}\\}} \\\\\n",
    "z_{ijd} = \\gamma_d \\cdot \\frac{x_{ijd} - \\mu_d}{\\sigma_d} + \\beta_d,\\\\\n",
    "$$\n",
    "\n",
    "with the expectation and variance taken across both the data samples of the batch and the spatial dimensions.\n",
    "\n",
    "The $\\mu_d$ and $\\sigma_d$ values are computed on the actual mini-batch during training, but at test-time they are fixed, so that the prediction of the final system on a given sample does not depend on other samples in the mini-batch. To obtain the fixed values for test-time use, one needs to maintain moving statistics over the activations during training. This can be a bit tricky to implement from scratch, but luckily this is now implemented in all popular frameworks, including PyTorch.\n",
    "\n",
    "When applying BatchNorm, it is not necessary to use biases in the previous convolutional layer. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f1772ceb21ca3d08ab31a044983550e",
     "grade": true,
     "grade_id": "cell-0178a85974e9358e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*POINTS: 1*\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "Because BatchNorm normalizes the output by subtracting the mean (which cancels out the bias) and then scales and shifts it using its own parameters (ùõæ and ùõΩ), the bias term in the previous convolutional layer becomes unnecessary and redundant. Hence, it is common practice to omit the bias term in layers preceding a BatchNorm layer to simplify the model and reduce the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c42e515e3a912d8315bb91c79597216e",
     "grade": false,
     "grade_id": "cell-1a1a3ac922ed7e52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create a modified version of the previous model using batch normalization between each convolution and the corresponding activation. Use the `bias` argument of `nn.Conv2d` according to the above finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "690f6b764872cff92a8ccb1ef727d884",
     "grade": true,
     "grade_id": "cell-83b754b10f9a5f09",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/25\n",
      "Finished epoch 1/25\n",
      "Starting epoch 2/25\n",
      "Finished epoch 2/25\n",
      "Starting epoch 3/25\n",
      "Finished epoch 3/25\n",
      "Starting epoch 4/25\n",
      "Finished epoch 4/25\n",
      "Starting epoch 5/25\n",
      "Finished epoch 5/25\n",
      "Starting epoch 6/25\n",
      "Finished epoch 6/25\n",
      "Starting epoch 7/25\n",
      "Finished epoch 7/25\n",
      "Starting epoch 8/25\n",
      "Finished epoch 8/25\n",
      "Starting epoch 9/25\n",
      "Finished epoch 9/25\n",
      "Starting epoch 10/25\n",
      "Finished epoch 10/25\n",
      "Starting epoch 11/25\n",
      "Finished epoch 11/25\n",
      "Starting epoch 12/25\n",
      "Finished epoch 12/25\n",
      "Starting epoch 13/25\n",
      "Finished epoch 13/25\n",
      "Starting epoch 14/25\n",
      "Finished epoch 14/25\n",
      "Starting epoch 15/25\n",
      "Finished epoch 15/25\n",
      "Starting epoch 16/25\n",
      "Finished epoch 16/25\n",
      "Starting epoch 17/25\n",
      "Finished epoch 17/25\n",
      "Starting epoch 18/25\n",
      "Finished epoch 18/25\n",
      "Starting epoch 19/25\n",
      "Finished epoch 19/25\n",
      "Starting epoch 20/25\n",
      "Finished epoch 20/25\n",
      "Starting epoch 21/25\n",
      "Finished epoch 21/25\n",
      "Starting epoch 22/25\n",
      "Finished epoch 22/25\n",
      "Starting epoch 23/25\n",
      "Finished epoch 23/25\n",
      "Starting epoch 24/25\n",
      "Finished epoch 24/25\n",
      "Starting epoch 25/25\n",
      "Finished epoch 25/25\n"
     ]
    }
   ],
   "source": [
    "# POINTS: 3\n",
    "\n",
    "cnn_batchnorm = nn.Sequential(\n",
    "    # YOUR CODE HERE\n",
    "    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=4096, out_features=10)\n",
    ")\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "if USE_GPU:\n",
    "    cnn_batchnorm.cuda()\n",
    "\n",
    "train_model(\n",
    "    model=cnn_batchnorm,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    opt=optim.Adam(cnn_batchnorm.parameters(), lr=5e-4),\n",
    "    logdir='cnn_batchnorm',\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7ac28f2a6faced5ad48faa2f43224f6",
     "grade": false,
     "grade_id": "cell-57707594f666ee77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Strided Convolutions\n",
    "\n",
    "Max-pooling is a popular technique for reducing the spatial dimensionality\n",
    "of the outputs from conv layers. Another way to reduce dimensionality is striding. For an argument why this may be similarly effective, see [Springenberg et al., ICLRW'15](https://arxiv.org/pdf/1412.6806.pdf).\n",
    "\n",
    "Now create a model using the same architecture as before, with the difference of\n",
    "removing the max-pooling layers and increasing the stride parameter of the conv layers to $2 \\times 2$ in the spatial dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "671e529976d935a56918ea2030028bff",
     "grade": true,
     "grade_id": "cell-34f5d6a1166b46fa",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/25\n",
      "Finished epoch 1/25\n",
      "Starting epoch 2/25\n",
      "Finished epoch 2/25\n",
      "Starting epoch 3/25\n",
      "Finished epoch 3/25\n",
      "Starting epoch 4/25\n",
      "Finished epoch 4/25\n",
      "Starting epoch 5/25\n",
      "Finished epoch 5/25\n",
      "Starting epoch 6/25\n",
      "Finished epoch 6/25\n",
      "Starting epoch 7/25\n",
      "Finished epoch 7/25\n",
      "Starting epoch 8/25\n",
      "Finished epoch 8/25\n",
      "Starting epoch 9/25\n",
      "Finished epoch 9/25\n",
      "Starting epoch 10/25\n",
      "Finished epoch 10/25\n",
      "Starting epoch 11/25\n",
      "Finished epoch 11/25\n",
      "Starting epoch 12/25\n",
      "Finished epoch 12/25\n",
      "Starting epoch 13/25\n",
      "Finished epoch 13/25\n",
      "Starting epoch 14/25\n",
      "Finished epoch 14/25\n",
      "Starting epoch 15/25\n",
      "Finished epoch 15/25\n",
      "Starting epoch 16/25\n",
      "Finished epoch 16/25\n",
      "Starting epoch 17/25\n",
      "Finished epoch 17/25\n",
      "Starting epoch 18/25\n",
      "Finished epoch 18/25\n",
      "Starting epoch 19/25\n",
      "Finished epoch 19/25\n",
      "Starting epoch 20/25\n",
      "Finished epoch 20/25\n",
      "Starting epoch 21/25\n",
      "Finished epoch 21/25\n",
      "Starting epoch 22/25\n",
      "Finished epoch 22/25\n",
      "Starting epoch 23/25\n",
      "Finished epoch 23/25\n",
      "Starting epoch 24/25\n",
      "Finished epoch 24/25\n",
      "Starting epoch 25/25\n",
      "Finished epoch 25/25\n"
     ]
    }
   ],
   "source": [
    "# POINTS: 2\n",
    "\n",
    "cnn_strides = nn.Sequential(\n",
    "    # YOUR CODE HERE\n",
    "    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=4096, out_features=10)\n",
    "    #raise NotImplementedError()\n",
    ")\n",
    "\n",
    "if USE_GPU:\n",
    "    cnn_strides.cuda()\n",
    "\n",
    "train_model(\n",
    "    model=cnn_strides,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    opt=optim.Adam(cnn_strides.parameters(), lr=5e-4),\n",
    "    logdir='cnn_strides',\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb27ebaacf2db8e350735285db940944",
     "grade": false,
     "grade_id": "cell-cbb6e2eeb2dcc66e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What differences do you notice when training this new network?\n",
    "What is a clear advantage of using strides?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4252bf0c44f951d77c444af2ef1162f5",
     "grade": true,
     "grade_id": "cell-12ee310e8234b8f8",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*POINTS: 1*\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "\n",
    "\n",
    "In the network where I used strides, there were less parameters, hence the training process was much faster. Strided convolutions reduce the spatial dimensions of feature maps without adding additional parameters or operation layer, and therefore they are computationally efficient since they integrate the downsampling process directly into the convolution operation, reducing the number of operations and memory footprint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10c6a5679d00652a999570b133d4f792",
     "grade": false,
     "grade_id": "cell-bc32808a463893fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Global Pooling\n",
    "\n",
    "The above network ends in a `Flatten` layer followed by a `Linear` layer, in which the number of weights depends on the input size. This means that testing can only be performed on the exact same image size. Several architectures employ a (spatial) **global average pooling layer** to produce a vector of fixed size describing the whole image.\n",
    "\n",
    "Implement the forward pass of such a layer below. The input `x` has size `[batch_size, channels, height, width]`. The mean must be computed across the last two dimensions, such that the result `pooled` has a size of `[batch_size, channels]`. Note, that you cannot use numpy for this as PyTorch is not able to backpropagate through numpy functions. There are however corresponding PyTorch functions for almost all numpy functions.\n",
    "\n",
    "This layer can now replace the flattening operation from the previous network. However, the units before the average pooling need to have a large enough receptive field, otherwise the model will not work well. Therefore, compared with the previous model, remove the `Flatten` layer and instead add a third Conv-BatchNorm-ReLU combination. Then add `GlobalAvgPool2d` and a final `Linear` layer which returns $10$ values per sample instead of $64$.\n",
    "\n",
    "Train it and see if it reaches similar accuracy to the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35eb344df1aa58334ad9b51d0b7bf7f4",
     "grade": true,
     "grade_id": "cell-384e1eaafbd3f3b6",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/25\n",
      "Finished epoch 1/25\n",
      "Starting epoch 2/25\n",
      "Finished epoch 2/25\n",
      "Starting epoch 3/25\n",
      "Finished epoch 3/25\n",
      "Starting epoch 4/25\n",
      "Finished epoch 4/25\n",
      "Starting epoch 5/25\n",
      "Finished epoch 5/25\n",
      "Starting epoch 6/25\n",
      "Finished epoch 6/25\n",
      "Starting epoch 7/25\n",
      "Finished epoch 7/25\n",
      "Starting epoch 8/25\n",
      "Finished epoch 8/25\n",
      "Starting epoch 9/25\n",
      "Finished epoch 9/25\n",
      "Starting epoch 10/25\n",
      "Finished epoch 10/25\n",
      "Starting epoch 11/25\n",
      "Finished epoch 11/25\n",
      "Starting epoch 12/25\n",
      "Finished epoch 12/25\n",
      "Starting epoch 13/25\n",
      "Finished epoch 13/25\n",
      "Starting epoch 14/25\n",
      "Finished epoch 14/25\n",
      "Starting epoch 15/25\n",
      "Finished epoch 15/25\n",
      "Starting epoch 16/25\n",
      "Finished epoch 16/25\n",
      "Starting epoch 17/25\n",
      "Finished epoch 17/25\n",
      "Starting epoch 18/25\n",
      "Finished epoch 18/25\n",
      "Starting epoch 19/25\n",
      "Finished epoch 19/25\n",
      "Starting epoch 20/25\n",
      "Finished epoch 20/25\n",
      "Starting epoch 21/25\n",
      "Finished epoch 21/25\n",
      "Starting epoch 22/25\n",
      "Finished epoch 22/25\n",
      "Starting epoch 23/25\n",
      "Finished epoch 23/25\n",
      "Starting epoch 24/25\n",
      "Finished epoch 24/25\n",
      "Starting epoch 25/25\n",
      "Finished epoch 25/25\n"
     ]
    }
   ],
   "source": [
    "# POINTS: 4\n",
    "\n",
    "class GlobalAvgPool2d(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        # Perform global average pooling\n",
    "        pooled = F.adaptive_avg_pool2d(x, (1, 1))  # Pooling across the entire spatial dimensions\n",
    "        pooled = torch.flatten(pooled, 1)  # Flatten the tensor except for the batch dimension\n",
    "        #raise NotImplementedError()\n",
    "        return pooled\n",
    "\n",
    "\n",
    "cnn_global_pool = nn.Sequential(\n",
    "    # YOUR CODE HERE\n",
    "    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    \n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    \n",
    "    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    \n",
    "    GlobalAvgPool2d(),  # Apply global average pooling\n",
    "    nn.Linear(128, 10)  # Final linear layer with 10 output classes\n",
    "\n",
    "    #raise NotImplementedError()\n",
    ")\n",
    "\n",
    "if USE_GPU:\n",
    "    cnn_global_pool.cuda()\n",
    "\n",
    "train_model(\n",
    "    model=cnn_global_pool,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    opt=optim.Adam(cnn_global_pool.parameters(), lr=5e-4),\n",
    "    logdir='cnn_global_pool',\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee6de3f7f259be2d205f59714faefb0a",
     "grade": false,
     "grade_id": "cell-527e589864d3a660",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Which network has more parameters, this or the previous one?\n",
    "\n",
    "What is the size of the receptive field of the units in the layer directly before the global average pooling? (Remember: the receptive field of a particular unit (neuron) is the area of the *input image* that can influence the activation of this given unit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d83ca3fe01cd1485eea9363077f637aa",
     "grade": true,
     "grade_id": "cell-a36f95947bd5ba86",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*POINTS: 2*\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "The `cnn_global_pool` network has more parameters.\n",
    "\n",
    "\n",
    "The receptive field of `cnn_global_pool` is $7$ \\\n",
    "First convolutional layer: $3$ \\\n",
    "Second convolutional layer: $3+(3-1)x1 = 5$ \\\n",
    "Third convolutional layer: $5 + (3-1)x1 = 7$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51b04c4d2bd7fd3084d24223fa69f427",
     "grade": false,
     "grade_id": "cell-cfc670e8ea147092",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Residual Networks\n",
    "\n",
    "ResNet was introduced by [He et al. in 2016](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) and is still popular today.\n",
    "\n",
    "It consists of blocks like the following:\n",
    "\n",
    "![ResNet Block](resnet_block.png)\n",
    "\n",
    "Each of these so-called *residual blocks* only have to predict a *residual* (in plain words: the \"rest\", the \"leftover\") that will be added on top of its input.\n",
    "In other words, the block outputs how much each feature needs to be changed in order to enhance the representation compared to the previous block.\n",
    "\n",
    "There are several ways to combine residual blocks into *residual networks* (ResNets). In the following, we consider ResNet-v1, as used for the CIFAR-10 benchmark in the original ResNet paper (it is simpler compared to the full model that they used for the much larger ImageNet benchmark).\n",
    "\n",
    "Section 4.2. of the paper describes this architecture as follows: \"*The first layer is 3√ó3 convolutions. Then we use a stack of 6n layers with 3√ó3 convolutions on the feature maps of sizes {32, 16, 8} respectively, with 2n layers for each feature map size. The numbers of filters are {16, 32, 64} respectively. The subsampling is performed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. [...] When shortcut connections are used, they are connected to the pairs of 3√ó3 layers (totally 3n shortcuts). On this dataset we use identity shortcuts in all cases.*\"\n",
    "\n",
    "Further, they use L2 regularization for training (a standard tool to combat overfitting). This penalizes weights with large magnitude by adding an additional term to the cost function, besides the cross-entropy. The overall function to optimize becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE} + \\frac{\\lambda}{2} \\sum_{w\\in\\text{weights}} w^2,\n",
    "$$\n",
    "\n",
    "and in this paper $\\lambda=10^{-4}$.\n",
    "\n",
    "Use the explanation above to complete the `layers`-list in the `ResNet`-class below. Note, that the first layer is already added and remember that the final softmax has to be omitted. The `ResNetBlock` already implements the above figure, i.e. contains $2$ convolutions.\n",
    "\n",
    "Weight decay is already added by setting the corresponding attribute of the opimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c857feab77578ca1b11a6a8d8940e16",
     "grade": true,
     "grade_id": "cell-78512b7752e9d7d2",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 6\n",
    "\n",
    "class ResNetBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "        # The shortcut connection is just the identity. If feature\n",
    "        # channel counts differ between input and output, zero\n",
    "        # padding is used to match the depths. This is implemented\n",
    "        # by a convolution with the following fixed weight:\n",
    "        self.pad_weight = nn.Parameter(\n",
    "            torch.eye(out_channels, in_channels)[:, :, None, None],\n",
    "            requires_grad=False\n",
    "        )\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = self.f(x)\n",
    "        # We apply the padding weight using torch.functional.conv2d\n",
    "        # which allows us to use a custom weight matrix.\n",
    "        x = F.conv2d(x, self.pad_weight, stride=self.stride)\n",
    "        return self.activation(x + r)\n",
    "    \n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self, num_layers=8, in_channels=3, out_features=10):\n",
    "        super().__init__()\n",
    "        if (num_layers - 2) % 6 != 0:\n",
    "            raise ValueError('n_layers should be 6n+2')\n",
    "        n = (num_layers - 2) // 6\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        first_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        layers.append(first_layer)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        for _ in range(n):\n",
    "            layers.append(ResNetBlock(16, 16))\n",
    "        \n",
    "        for _ in range(n):\n",
    "            layers.append(ResNetBlock(16, 32, stride=2))  # Downsampling\n",
    "        \n",
    "        for _ in range(n):\n",
    "            layers.append(ResNetBlock(32, 64, stride=2))  # Downsampling\n",
    "        \n",
    "        # Global average pooling\n",
    "        layers.append(nn.AdaptiveAvgPool2d(1))\n",
    "        \n",
    "        # Flatten before the final fully connected layer\n",
    "        layers.append(nn.Flatten())\n",
    "        \n",
    "        # Final fully connected layer (without softmax)\n",
    "        layers.append(nn.Linear(64, out_features))\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43566b9d8b66932b9ae455e7ac3795a9",
     "grade": false,
     "grade_id": "cell-ac38cfcbccbc3e34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Train a Resnet with 8 layers on the CIFAR-10 dataset for 50 epochs. As a rough idea, it will take less than 15 minutes with a good GPU, but on a CPU it can take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34b787795784e941368bcf5d3681f716",
     "grade": false,
     "grade_id": "cell-e3b3f6d8fa93f028",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss: 1.626, train accuracy: 0.411, test loss: 1.371, test accuracy: 0.504, time: 0:00:30\n",
      "Epoch 2 | train loss: 1.270, train accuracy: 0.546, test loss: 1.171, test accuracy: 0.580, time: 0:00:53\n",
      "Epoch 3 | train loss: 1.107, train accuracy: 0.606, test loss: 1.122, test accuracy: 0.589, time: 0:01:15\n",
      "Epoch 4 | train loss: 1.024, train accuracy: 0.638, test loss: 1.057, test accuracy: 0.626, time: 0:01:38\n",
      "Epoch 5 | train loss: 0.961, train accuracy: 0.659, test loss: 1.017, test accuracy: 0.637, time: 0:02:00\n",
      "Epoch 6 | train loss: 0.919, train accuracy: 0.676, test loss: 0.925, test accuracy: 0.669, time: 0:02:21\n",
      "Epoch 7 | train loss: 0.886, train accuracy: 0.688, test loss: 0.839, test accuracy: 0.701, time: 0:02:43\n",
      "Epoch 8 | train loss: 0.851, train accuracy: 0.701, test loss: 0.966, test accuracy: 0.668, time: 0:03:04\n",
      "Epoch 9 | train loss: 0.827, train accuracy: 0.710, test loss: 0.880, test accuracy: 0.687, time: 0:03:26\n",
      "Epoch 10 | train loss: 0.808, train accuracy: 0.714, test loss: 0.828, test accuracy: 0.706, time: 0:03:48\n",
      "Epoch 11 | train loss: 0.781, train accuracy: 0.726, test loss: 0.878, test accuracy: 0.700, time: 0:04:10\n",
      "Epoch 12 | train loss: 0.765, train accuracy: 0.731, test loss: 0.776, test accuracy: 0.726, time: 0:04:31\n",
      "Epoch 13 | train loss: 0.747, train accuracy: 0.738, test loss: 0.778, test accuracy: 0.730, time: 0:04:54\n",
      "Epoch 14 | train loss: 0.729, train accuracy: 0.747, test loss: 0.809, test accuracy: 0.720, time: 0:05:21\n",
      "Epoch 15 | train loss: 0.716, train accuracy: 0.750, test loss: 0.749, test accuracy: 0.739, time: 0:05:50\n",
      "Epoch 16 | train loss: 0.702, train accuracy: 0.756, test loss: 0.815, test accuracy: 0.712, time: 0:06:26\n",
      "Epoch 17 | train loss: 0.690, train accuracy: 0.758, test loss: 0.722, test accuracy: 0.744, time: 0:07:00\n",
      "Epoch 18 | train loss: 0.680, train accuracy: 0.764, test loss: 0.687, test accuracy: 0.757, time: 0:07:41\n",
      "Epoch 19 | train loss: 0.661, train accuracy: 0.771, test loss: 0.672, test accuracy: 0.763, time: 0:08:20\n",
      "Epoch 20 | train loss: 0.655, train accuracy: 0.774, test loss: 0.696, test accuracy: 0.757, time: 0:08:56\n",
      "Epoch 21 | train loss: 0.644, train accuracy: 0.775, test loss: 0.672, test accuracy: 0.769, time: 0:09:33\n",
      "Epoch 22 | train loss: 0.633, train accuracy: 0.781, test loss: 0.712, test accuracy: 0.756, time: 0:10:11\n",
      "Epoch 23 | train loss: 0.623, train accuracy: 0.783, test loss: 0.741, test accuracy: 0.736, time: 0:10:49\n",
      "Epoch 24 | train loss: 0.616, train accuracy: 0.787, test loss: 0.650, test accuracy: 0.772, time: 0:11:24\n",
      "Epoch 25 | train loss: 0.611, train accuracy: 0.788, test loss: 0.690, test accuracy: 0.763, time: 0:12:00\n",
      "Epoch 26 | train loss: 0.605, train accuracy: 0.790, test loss: 0.747, test accuracy: 0.745, time: 0:12:35\n",
      "Epoch 27 | train loss: 0.592, train accuracy: 0.795, test loss: 0.645, test accuracy: 0.780, time: 0:13:03\n",
      "Epoch 28 | train loss: 0.588, train accuracy: 0.796, test loss: 0.610, test accuracy: 0.793, time: 0:13:28\n",
      "Epoch 29 | train loss: 0.579, train accuracy: 0.798, test loss: 0.608, test accuracy: 0.790, time: 0:13:57\n",
      "Epoch 30 | train loss: 0.571, train accuracy: 0.804, test loss: 0.621, test accuracy: 0.789, time: 0:14:26\n",
      "Epoch 31 | train loss: 0.566, train accuracy: 0.804, test loss: 0.616, test accuracy: 0.786, time: 0:14:55\n",
      "Epoch 32 | train loss: 0.565, train accuracy: 0.804, test loss: 0.630, test accuracy: 0.785, time: 0:15:24\n",
      "Epoch 33 | train loss: 0.561, train accuracy: 0.806, test loss: 0.610, test accuracy: 0.792, time: 0:15:53\n",
      "Epoch 34 | train loss: 0.553, train accuracy: 0.807, test loss: 0.579, test accuracy: 0.799, time: 0:16:22\n",
      "Epoch 35 | train loss: 0.549, train accuracy: 0.810, test loss: 0.558, test accuracy: 0.803, time: 0:16:51\n",
      "Epoch 36 | train loss: 0.543, train accuracy: 0.813, test loss: 0.635, test accuracy: 0.786, time: 0:17:20\n",
      "Epoch 37 | train loss: 0.539, train accuracy: 0.816, test loss: 0.553, test accuracy: 0.810, time: 0:17:48\n",
      "Epoch 38 | train loss: 0.535, train accuracy: 0.816, test loss: 0.632, test accuracy: 0.786, time: 0:18:17\n",
      "Epoch 39 | train loss: 0.531, train accuracy: 0.817, test loss: 0.612, test accuracy: 0.790, time: 0:18:45\n",
      "Epoch 40 | train loss: 0.527, train accuracy: 0.817, test loss: 0.624, test accuracy: 0.786, time: 0:19:14\n",
      "Epoch 41 | train loss: 0.520, train accuracy: 0.821, test loss: 0.560, test accuracy: 0.807, time: 0:19:43\n",
      "Epoch 42 | train loss: 0.520, train accuracy: 0.822, test loss: 0.575, test accuracy: 0.805, time: 0:20:13\n",
      "Epoch 43 | train loss: 0.518, train accuracy: 0.819, test loss: 0.574, test accuracy: 0.806, time: 0:24:45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-99d9c920285b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mlogdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'resnet'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtest_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;32m<ipython-input-15-7147ff18d845>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, opt, logdir, train_loader, test_loader, n_epochs, lr_scheduler)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mcorrect_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m  \u001b[1;31m# total number of correctly classified samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mn_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# number of batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# iterate over training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;31m# training step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mUSE_GPU\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenih\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenih\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenih\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenih\\Anaconda3\\envs\\myenv\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenih\\Anaconda3\\envs\\myenv\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenih\\Anaconda3\\envs\\myenv\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenih\\Anaconda3\\envs\\myenv\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenih\\Anaconda3\\envs\\myenv\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resnet = ResNet()\n",
    "if USE_GPU:\n",
    "    resnet.cuda()\n",
    "\n",
    "train_model(\n",
    "    model=resnet,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    opt=optim.Adam(resnet.parameters(), lr=5e-4, weight_decay=1e-4),\n",
    "    logdir='resnet',\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d30d6ddca7fb16235c3573be47b0266",
     "grade": false,
     "grade_id": "cell-434819020b48d6b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Learning Rate Decay\n",
    "\n",
    "Learning rate decay reduces the learning rate as the training progresses. Use the same settings as in the previous experiment, but this time create a `MultiStepLR`-scheduler and decrease the learning rate twice by a factor of 10 each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3eef0cab948ba25dcc7328a8811b7f4",
     "grade": true,
     "grade_id": "cell-f2042420b7d15963",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss: 1.665, train accuracy: 0.399, test loss: 1.398, test accuracy: 0.492, time: 0:00:21\n",
      "Epoch 2 | train loss: 1.313, train accuracy: 0.530, test loss: 1.250, test accuracy: 0.546, time: 0:00:44\n",
      "Epoch 3 | train loss: 1.157, train accuracy: 0.588, test loss: 1.216, test accuracy: 0.570, time: 0:01:05\n",
      "Epoch 4 | train loss: 1.060, train accuracy: 0.624, test loss: 1.023, test accuracy: 0.631, time: 0:01:27\n",
      "Epoch 5 | train loss: 0.987, train accuracy: 0.650, test loss: 1.063, test accuracy: 0.621, time: 0:01:49\n",
      "Epoch 6 | train loss: 0.941, train accuracy: 0.668, test loss: 0.923, test accuracy: 0.670, time: 0:02:10\n",
      "Epoch 7 | train loss: 0.896, train accuracy: 0.685, test loss: 0.921, test accuracy: 0.672, time: 0:02:32\n",
      "Epoch 8 | train loss: 0.863, train accuracy: 0.696, test loss: 0.828, test accuracy: 0.706, time: 0:02:54\n",
      "Epoch 9 | train loss: 0.828, train accuracy: 0.710, test loss: 0.843, test accuracy: 0.704, time: 0:03:15\n",
      "Epoch 10 | train loss: 0.802, train accuracy: 0.719, test loss: 0.870, test accuracy: 0.701, time: 0:03:37\n",
      "Epoch 11 | train loss: 0.779, train accuracy: 0.726, test loss: 0.817, test accuracy: 0.709, time: 0:03:59\n",
      "Epoch 12 | train loss: 0.760, train accuracy: 0.733, test loss: 0.829, test accuracy: 0.709, time: 0:04:21\n",
      "Epoch 13 | train loss: 0.736, train accuracy: 0.743, test loss: 0.777, test accuracy: 0.725, time: 0:04:42\n",
      "Epoch 14 | train loss: 0.721, train accuracy: 0.747, test loss: 0.731, test accuracy: 0.743, time: 0:05:04\n",
      "Epoch 15 | train loss: 0.714, train accuracy: 0.750, test loss: 0.790, test accuracy: 0.732, time: 0:05:26\n",
      "Epoch 16 | train loss: 0.695, train accuracy: 0.758, test loss: 0.780, test accuracy: 0.730, time: 0:05:47\n",
      "Epoch 17 | train loss: 0.680, train accuracy: 0.763, test loss: 0.773, test accuracy: 0.734, time: 0:06:16\n",
      "Epoch 18 | train loss: 0.672, train accuracy: 0.765, test loss: 0.698, test accuracy: 0.757, time: 0:06:45\n",
      "Epoch 19 | train loss: 0.658, train accuracy: 0.770, test loss: 0.660, test accuracy: 0.772, time: 0:07:22\n",
      "Epoch 20 | train loss: 0.650, train accuracy: 0.773, test loss: 0.696, test accuracy: 0.756, time: 0:07:58\n",
      "Epoch 21 | train loss: 0.640, train accuracy: 0.778, test loss: 0.682, test accuracy: 0.768, time: 0:08:32\n",
      "Epoch 22 | train loss: 0.632, train accuracy: 0.781, test loss: 0.717, test accuracy: 0.753, time: 0:09:07\n",
      "Epoch 23 | train loss: 0.622, train accuracy: 0.784, test loss: 0.662, test accuracy: 0.769, time: 0:09:42\n",
      "Epoch 24 | train loss: 0.616, train accuracy: 0.784, test loss: 0.725, test accuracy: 0.750, time: 0:10:17\n",
      "Epoch 25 | train loss: 0.609, train accuracy: 0.789, test loss: 0.640, test accuracy: 0.777, time: 0:10:50\n",
      "Epoch 26 | train loss: 0.604, train accuracy: 0.791, test loss: 0.696, test accuracy: 0.761, time: 0:11:25\n",
      "Epoch 27 | train loss: 0.592, train accuracy: 0.796, test loss: 0.675, test accuracy: 0.763, time: 0:12:01\n",
      "Epoch 28 | train loss: 0.587, train accuracy: 0.797, test loss: 0.661, test accuracy: 0.772, time: 0:12:37\n",
      "Epoch 29 | train loss: 0.580, train accuracy: 0.799, test loss: 0.695, test accuracy: 0.764, time: 0:13:16\n",
      "Epoch 30 | train loss: 0.577, train accuracy: 0.801, test loss: 0.676, test accuracy: 0.769, time: 0:13:53\n",
      "Epoch 31 | train loss: 0.570, train accuracy: 0.803, test loss: 0.625, test accuracy: 0.786, time: 0:14:30\n",
      "Epoch 32 | train loss: 0.561, train accuracy: 0.807, test loss: 0.630, test accuracy: 0.781, time: 0:15:08\n",
      "Epoch 33 | train loss: 0.557, train accuracy: 0.806, test loss: 0.689, test accuracy: 0.761, time: 0:15:45\n",
      "Epoch 34 | train loss: 0.554, train accuracy: 0.808, test loss: 0.638, test accuracy: 0.784, time: 0:16:22\n",
      "Train epoch 35, step 270/391        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-61e8cc2074a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mtest_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msched\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m )\n",
      "\u001b[1;32m<ipython-input-15-7147ff18d845>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, opt, logdir, train_loader, test_loader, n_epochs, lr_scheduler)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenih\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenih\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# POINTS: 2\n",
    "\n",
    "resnet_decay = ResNet()\n",
    "if USE_GPU:\n",
    "    resnet_decay.cuda()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Define optimizer Adam\n",
    "opt = optim.Adam(resnet_decay.parameters(), lr=5e-4)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "# Decrease learning rate twice by a factor of 10 each at specific epochs\n",
    "# milestones are epochs at which to divide learning rate by 10\n",
    "sched = MultiStepLR(opt, milestones=[50, 75], gamma=0.1)\n",
    "#raise NotImplementedError()\n",
    "\n",
    "train_model(\n",
    "    model=resnet_decay,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    opt=opt,\n",
    "    logdir='resnet_decay',\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    lr_scheduler=sched\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e34156f290a21658236e3b147ffe4f80",
     "grade": false,
     "grade_id": "cell-2a7fd7a5404ef569",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Positional Encoding\n",
    "\n",
    "After the ResNet, we will now have a look at a more recent architecture, the Vision Transformer by [Dosovitskiy et al., 2020](https://arxiv.org/pdf/2010.11929.pdf).\n",
    "This model subdivides the image into patches and feeds these into a transformer.\n",
    "A CNN knows the spatial relationships between features very well, but a transformer does not.\n",
    "Without any further processing, each patch would become a token and each token would only differ by its contents, but would not differ due to the pixel positions.\n",
    "This is not wanted of course, as the location in images matters.\n",
    "For this reason, we need to add a positional encoding to the tokens, such that e.g. the patch in the top left always receives the same encoding and thus the model is able to gather features using positions.\n",
    "\n",
    "If each token $i$ is represented by $d$ features, that the $j$-th value is defined as\n",
    "\n",
    "$$\n",
    "p_{i,j} = \n",
    "\\begin{cases}\n",
    "\\sin\\left(\\frac{i}{10000^{\\frac{j}{d}}}\\right) & \\; \\text{if } j \\text{ is even}\\\\\n",
    "\\cos\\left(\\frac{i}{10000^{\\frac{j-1}{d}}}\\right) & \\; \\text{if } j \\text{ is odd}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To avoid recomputing this expression for every evaluation, we want to precompute it once for some $i$ and $j$.\n",
    "If you implemented this correctly, each row of the image looks different from every other row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d142c079a07af305aec9b13dabf5f64e",
     "grade": true,
     "grade_id": "cell-e442bcc53217f8e5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# POINTS: 2\n",
    "\n",
    "POS_ENC_CACHE_T = 100\n",
    "POS_ENC_CACHE_D = 300\n",
    "pos_enc_cached = torch.zeros(POS_ENC_CACHE_T, POS_ENC_CACHE_D)\n",
    "# YOUR CODE HERE\n",
    "# Precompute the divisor for each feature dimension to simplify the calculation\n",
    "divisors = torch.pow(10000, torch.arange(0, POS_ENC_CACHE_D, 2) / POS_ENC_CACHE_D)\n",
    "\n",
    "# Compute the positional encodings\n",
    "for i in range(POS_ENC_CACHE_T):\n",
    "    for j in range(0, POS_ENC_CACHE_D, 2):\n",
    "        pos_enc_cached[i, j] = np.sin(i / divisors[j // 2])\n",
    "        if j + 1 < POS_ENC_CACHE_D:\n",
    "            pos_enc_cached[i, j + 1] = np.cos(i / divisors[j // 2])\n",
    "\n",
    "#raise NotImplementedError()\n",
    "plt.imshow(pos_enc_cached)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37440a0c51dd2cb6781e0ce6363d720b",
     "grade": false,
     "grade_id": "cell-0015c81d69e75b6e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Why do we use addition instad of concatenation?\n",
    "E.g. instead of 300 image features added to 300 positional features, we coul concatenate 150 image features to 150 positional features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a631f5d72ddf6d091de3484d56c71716",
     "grade": true,
     "grade_id": "cell-811bb582df184b94",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*POINTS: 1*\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "Positional encoding in transformers does not introduce additional non-linearity to the token embeddings. Instead, positional encoding acts more like a linear transformation. This is because any change in position leads to a linear change in the token embedding.\n",
    "\n",
    "The property shouldn‚Äôt add additional non-linearity to the token embedding, but instead acts more like a linear transformation, since any change in position changes the the embedding linearly. In my intuition this should also enable easy separation of positional vs token information\n",
    "\n",
    "So, by adding the embeddings, we‚Äôre not losing any information. Instead, we‚Äôre allowing the model to decide how best to use the information from the token embeddings and the positional encodings. This is more efficient than concatenation, which would increase the dimensionality and the number of parameters the model needs to learn.\n",
    "\n",
    "Addition retains the dimensionality of the original embeddings (positional and word) however, concatenate can double the dimensions thus needing an extra layer. There are many ways this extra dimensionality can be a problem like introducing noise, needing extra layers to just understand this dimensions, more parameters to train and has a higher chance of over fitting. Thus adding avoids the complexity and retains the initial semantics and positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "536e7ffdeba921fc117b98bb0c77a349",
     "grade": false,
     "grade_id": "cell-7cc85370d7a0e8a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The forward function of the following module receives tokens as input which have shape $(N \\times T \\times D)$, where $N$ is the batch size, $T$ the number of tokens and $D$ the number of features.\n",
    "Complete the implementation by adding (not concatenating!) the correct part of the precomputed positional embedding to the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99c4a0ec15f88fe9f9c4f15b476805b2",
     "grade": true,
     "grade_id": "cell-d3bb8a2e7c802bc1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 1\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # YOUR CODE HERE\n",
    "        positional_encoding_tokens = tokens + self.positional_encoding[:, :tokens.size(1), :]\n",
    "        \n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2add0a0c377957d73f620e1c77f7c984",
     "grade": false,
     "grade_id": "cell-9c617c1eac0a89f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, im_shape, patch_size, out_dim):\n",
    "        super().__init__()\n",
    "        assert (\n",
    "            im_shape[1] % patch_size[0] == 0\n",
    "        ), \"image height has to be divisible by patch height\"\n",
    "        assert (\n",
    "            im_shape[2] % patch_size[1] == 0\n",
    "        ), \"image width has to be divisible by patch width\"\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.input_dim = int(im_shape[0] * patch_size[0] * patch_size[1])  # size of a flattened patch\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.linear_mapper = nn.Linear(self.input_dim, self.out_dim)  # embeds a patch into a vector\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, self.out_dim))  # encodes the class at the end\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "\n",
    "    def patchify(self, images):\n",
    "        num_patches = (images.shape[2] // self.patch_size[0]) * (\n",
    "            images.shape[3] // self.patch_size[1]\n",
    "        )\n",
    "\n",
    "        split_H = images.unfold(2, self.patch_size[0], self.patch_size[0])\n",
    "        split_HW = split_H.unfold(3, self.patch_size[1], self.patch_size[1])\n",
    "        reordered = split_HW.permute(0, 2, 3, 1, 4, 5)\n",
    "        reshaped = reordered.reshape(images.size(0), num_patches, -1)\n",
    "        return reshaped\n",
    "\n",
    "    def forward(self, images):\n",
    "        patches = self.patchify(images)  # convert image to patches\n",
    "\n",
    "\n",
    "        tokens = self.linear_mapper(patches)  # embed patches\n",
    "\n",
    "        # add classification token to every sample of the batch\n",
    "        tokens = torch.stack(\n",
    "            [torch.vstack((self.cls_token, tokens[i])) for i in range(len(tokens))]\n",
    "        )\n",
    "\n",
    "        tokens = self.positional_encoding(tokens)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd3b0ef0b7cef3d3525990457e205bbf",
     "grade": false,
     "grade_id": "cell-8327dc96884ec57b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The function `patchify` takes images of shape $(N \\times C \\times H \\times W)$ as input and outputs tokens of shape $(N \\times T \\times D)$.\n",
    "Using $H_p$ and $W_p$ as height and width of the patches and $H_n$ and $W_n$ as the number of patches across height and width, give the size of each of the tensors in `patchify`.\n",
    "Also, provide formulas to compute $T$ and $D$ given the other terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "558f07b55d751980eb205c10d3bd7082",
     "grade": true,
     "grade_id": "cell-40fb9e0d8f7b620e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "*POINTS: 0*\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "Sizes of Tensors \\\n",
    "Input Tensor: ( ùëÅ √ó ùê∂ √ó ùêª √ó ùëä ) \\\n",
    "Output Tensor: ( ùëÅ √ó ùëá √ó ùê∑ )\n",
    "\n",
    "Formulas \\\n",
    "$ùëá = ( ùêª ùêª_ùëù) √ó (ùëä ùëä_ùëù)$ \\\n",
    "$ùê∑ = ùê∂ √ó ùêª_ùëù √ó  ùëä_ùëù $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6aa2bfd2c2a5dc7db214a751fc2b8d16",
     "grade": false,
     "grade_id": "cell-ddadd0b085f0d4c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "embedding = Embedding((3, 32, 32), (4, 4), 128)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "patch_size = (4, 4)  # (H, W)\n",
    "\n",
    "patches = embedding.patchify(images)\n",
    "patch_grid = make_grid(\n",
    "    patches[0].view(patches.shape[1], images.shape[1], patch_size[0], patch_size[1]),\n",
    "    padding=1,\n",
    "    normalize=True,\n",
    "    pad_value=1,\n",
    "    nrow=images.shape[3] // patch_size[0],\n",
    ").cpu()\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(5, 5))\n",
    "axarr[0].axis(\"off\")\n",
    "axarr[0].set_title(\"Original\")\n",
    "axarr[0].imshow(np.transpose(images[0]/2+0.5, (1, 2, 0)), cmap=\"gray\")\n",
    "axarr[1].axis(\"off\")\n",
    "axarr[1].set_title(\"Patches\")\n",
    "axarr[1].imshow(np.transpose(patch_grid, (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86b89d2f24540be3913f186ba18a227d",
     "grade": false,
     "grade_id": "cell-e3021bbaca7c832d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Self-Attention\n",
    "\n",
    "Next, we implement the self-attention mechanism, which is defined as\n",
    "$$\n",
    "\\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)V,\n",
    "$$\n",
    "where $Q$, $K$ and $V$ are computed by applying a dense layer to the input and $d$ is the number of features.\n",
    "Complete the following module.\n",
    "Note that this module is used for multi-head self-attention, so while the input to has `input_dim` dimensions, the attention is computed using `head_dim` dimensions and the single head outputs `head_dim` dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7248d640952e14b3e1e63392899e3b5",
     "grade": true,
     "grade_id": "cell-c68110c1d86173ef",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 4\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, input_dim, head_dim):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.query = nn.Linear(input_dim, head_dim)\n",
    "        self.key = nn.Linear(input_dim, head_dim)\n",
    "        self.value = nn.Linear(input_dim, head_dim)\n",
    "        # raise NotImplementedError()\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # YOUR CODE HERE\n",
    "        Q = self.query(tokens)\n",
    "        K = self.key(tokens)\n",
    "        V = self.value(tokens)\n",
    "\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(Q.shape[-1], dtype=torch.float32))\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return attention_output\n",
    "        # raise NotImplementedError()\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, dim):\n",
    "        super().__init__()\n",
    "        if dim % n_heads != 0:\n",
    "            raise ValueError('dim must be divisable by n_heads')\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "\n",
    "        self.heads = nn.ModuleList()\n",
    "        for _ in range(self.n_heads):\n",
    "            self.heads.append(AttentionHead(self.dim, self.head_dim))\n",
    "\n",
    "        self.out_projection = nn.Linear(self.n_heads * self.head_dim, self.dim)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        result = []\n",
    "        for seq in tokens:\n",
    "            result.append(torch.hstack([head(seq) for head in self.heads]))\n",
    "        result = torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
    "        return self.out_projection(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca10a4586a8ced0cf3bb3ffa9f24826e",
     "grade": false,
     "grade_id": "cell-e6d8f813b6d1c7e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Transformer\n",
    "\n",
    "We will now construct the transformer using transformer blocks.\n",
    "Each block performs the following steps:\n",
    "$$ \\mathbf z'_\\ell = \\text{MSA}(\\text{LN}(\\mathbf z_{\\ell-1}))+\\mathbf z_{\\ell-1} $$\n",
    "$$ \\mathbf z_\\ell = \\text{MLP}(\\text{LN}(\\mathbf z'_\\ell))+\\mathbf z'_\\ell $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f79533f89a4c117c9a682df3caf7e00",
     "grade": true,
     "grade_id": "cell-b2469e3e4ebba160",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 2\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_heads, dim, mlp_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "\n",
    "        # 1) Layer normalization 1\n",
    "        self.ln1 = nn.LayerNorm(self.dim)\n",
    "\n",
    "        # 2) Multi-head Self-Attention (MSA)\n",
    "        self.msa = MultiHeadSelfAttention(self.n_heads, self.dim)\n",
    "\n",
    "        # 3) Layer normalization 2\n",
    "        self.ln2 = nn.LayerNorm(self.dim)\n",
    "\n",
    "        # 4) MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.mlp_dim, self.dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Layer normalization 1\n",
    "        z_norm = self.ln1(tokens)\n",
    "\n",
    "        # Multi-head Self-Attention and residual connection\n",
    "        z_prime = self.msa(z_norm) + tokens\n",
    "\n",
    "        # Layer normalization 2\n",
    "        z_prime_norm = self.ln2(z_prime)\n",
    "\n",
    "        # MLP and residual connection\n",
    "        z = self.mlp(z_prime_norm) + z_prime\n",
    "\n",
    "        return z\n",
    "    \n",
    "        # raise NotImplementedError()\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, n_heads, dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            layer = TransformerBlock(\n",
    "                n_heads=n_heads,\n",
    "                dim=dim,\n",
    "                mlp_dim=mlp_dim,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            tokens = layer(tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe8961ae9b4709cce88c240eb59d43b7",
     "grade": false,
     "grade_id": "cell-3c7b5b94947934bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ViT\n",
    "Now combine everything together:\n",
    "First create the tokens, then apply the transformer, extract the class token and apply the classification MLP to produce the final logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c88fc3553661ee9a3cf28a6c9dd9b25",
     "grade": true,
     "grade_id": "cell-c71a046d6f8e4bb8",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 3\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        im_shape,\n",
    "        patch_size,\n",
    "        dim=8,\n",
    "        num_layers=1,\n",
    "        n_heads=2,\n",
    "        mlp_dim=8,\n",
    "        out_dim=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "\n",
    "        # 1) Embedding\n",
    "        self.embedding = Embedding(\n",
    "            im_shape=im_shape,\n",
    "            patch_size=patch_size,\n",
    "            out_dim=dim,\n",
    "        )\n",
    "\n",
    "        # 2) Transformer Encoder\n",
    "        self.encoder = Transformer(\n",
    "            num_layers=num_layers,\n",
    "            n_heads=n_heads,\n",
    "            dim=dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "        )\n",
    "\n",
    "        # 3) Classification MLP\n",
    "        self.cls_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # embedding -> encoding -> classification\n",
    "        # YOUR CODE HERE\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.reshape(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1).reshape(B, self.num_patches, self.patch_dim)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embedding\n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d78158fb0edf287f78456395fccdceae",
     "grade": false,
     "grade_id": "cell-aed3f4b8b8748fc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create a ViT-model and train it.\n",
    "Do you find hyper-parameters which result in a test accuracy of more than 50%?\n",
    "Note that this can take quite some time on CPU, depending on the parameters you choose.\n",
    "Do fewer epochs to find good parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6aa8f6995dc52c4db4a502750f857f40",
     "grade": true,
     "grade_id": "cell-a2df7753bb7cefe8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# POINTS: 2\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# ViT model definition\n",
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        im_shape,\n",
    "        patch_size,\n",
    "        dim=8,\n",
    "        num_layers=1,\n",
    "        n_heads=2,\n",
    "        mlp_dim=8,\n",
    "        out_dim=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "\n",
    "        # 1) Embedding\n",
    "        self.embedding = Embedding(\n",
    "            im_shape=im_shape,\n",
    "            patch_size=patch_size,\n",
    "            out_dim=dim,\n",
    "        )\n",
    "\n",
    "        # 2) Transformer Encoder\n",
    "        self.encoder = Transformer(\n",
    "            num_layers=num_layers,\n",
    "            n_heads=n_heads,\n",
    "            dim=dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "        )\n",
    "\n",
    "        # 3) Classification MLP\n",
    "        self.cls_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Step 1: Create tokens using the embedding layer\n",
    "        tokens = self.embedding(images)\n",
    "\n",
    "        # Step 2: Apply the transformer encoder\n",
    "        encoded_tokens = self.encoder(tokens)\n",
    "\n",
    "        # Step 3: Extract the class token (the first token)\n",
    "        cls_token = encoded_tokens[:, 0]\n",
    "\n",
    "        # Step 4: Apply the classification MLP to the class token\n",
    "        logits = self.cls_mlp(cls_token)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# Define the training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
