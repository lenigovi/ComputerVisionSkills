{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "Image segmentation can be classified into three categories:\n",
    "Semantic segmentation\n",
    "Instance segmentation\n",
    "Panoptic segmentation\n",
    "\n",
    "The document will cover the following topics:\n",
    "- Segmentation as pixel wise classification\n",
    "    - Probabilistic classification\n",
    "    - Mixture of Gaussians, EM\n",
    "\n",
    "- Segmentation as energy minimization\n",
    "    - Markov Random Fields\n",
    "    - Energy formulation\n",
    "\n",
    "- Graph cuts for image segmentaton\n",
    "    - Basic idea\n",
    "    - s-t Mincut Algorithm\n",
    "    - Extension to non-binary case\n",
    "\n",
    "\\\n",
    "The goal of segmentation is to identify the boundaries between different objects in an image. This topic is important because:\n",
    "- it is a more basic step for the convolutional filters in Neural Networks for extracting image features\n",
    "- it is the basis for many other image processing tasks such as object detection, object tracking, and image classification\n",
    "\n",
    "\\\n",
    "Definition of The Problem\\\n",
    "Identifying groups of pixels in the input which is the image. This is a semantic segmentation, this means all the pixels that belong to a title are grouped together. For example, semantically meaningful groups such as color similarity,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Segmentation Approaches:\n",
    "- Unsupervised Clustering\n",
    "    - Grouping what \"looks similar\"\n",
    "- Semantic Segmentation\n",
    "    - Learn a classifier to assign a semantic class $C_k$ to every pixel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To define the grouping semantics, we use feature space. Grayscale image pixels are classified based on intensity similarity as a 1D representation, while colored images are classified based on color value similarity as a 3D representation.\n",
    "\n",
    "A filter Bank of 24 filters allow us to operate in 24-dimension. \n",
    "\n",
    "\n",
    "\\\n",
    "Basically the method is to apply a threshold, above which is the foreground, and below is the background.\n",
    "\n",
    "\\\n",
    "Probabilistic Classification\n",
    "- Bayesian Classification\n",
    "    - Given a measurement $x$, what semantic class $C_k$ should we assign to a pixel?\n",
    "    - We must recall Bayes Decision Theory in this section:\n",
    "        - $P(C_k|x) = \\frac{p(x|C_k)p(C_k)}{\\sum_{j}p(x|C_j)p(C_j)} $\n",
    "        - where: \n",
    "            - $p(x|C_k)$ is the likelihood of the measurement $x$ having been generated by class $C_k$\n",
    "            -  $p(C_k)$ is the prior probability of class $C_k$\n",
    "    - In order to build a classifier, we can try either:\n",
    "        - Discriminative Methods: directly estimating the posterior\n",
    "        - Generative Methods: estimating likelihood and prior, and then using the Bayes Decision formula\n",
    "\n",
    "\\\n",
    "Mixture Model of Normal Distribution (Gaussian) MoG \\\n",
    "Will be referred as MoG hereon\n",
    "\n",
    "\n",
    "\\\n",
    "Expectation-Maximization (EM) Algorithm\n",
    " - E-Step: assign samples to mixture model components:\n",
    "    - $ \\pi_j \\gamma_j(x_n) $\n",
    "\n",
    "\n",
    "User Assisted Image Segmentation\n",
    "- User marks two regions for foreground and background\n",
    "- Learn a MoG model for the color values in each region\n",
    "- Use the models to classify all pixels by deciding for the class with the highest posterior probability\n",
    "\n",
    "### Pros of MoG, EM\n",
    "- \n",
    "\n",
    "It is called generative model because values can be generate from the distribution\n",
    "\n",
    "Cons\n",
    "- Local minima\n",
    "    - k-means is NP-hard (see: computational complexity theory, nondeterministic polynomial time) even with k=2\n",
    "- Initialization\n",
    "    - Often a good idea to start with some k-means iteration\n",
    "- Needs to know number of componens\n",
    "    - Solution: Model selection\n",
    "- Needs careful implementation to avoid numerical issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References for this Section\n",
    "\n",
    "---\n",
    "\n",
    "[1] v7labs, https://www.v7labs.com/blog/panoptic-segmentation-guide\n",
    "\n",
    "[2] fiveMinuteStats, EM, https://stephens999.github.io/fiveMinuteStats/intro_to_em.html\n",
    "\n",
    "[3] fiveMinuteStates, Mixture Models, https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html\n",
    "\n",
    "[4] neptune.ai, https://neptune.ai/blog/image-segmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have explored  bottom-up ways for segmentation, for which we examined individual pixels and neighborhoods to segment an image into regions, yet finding meaningful segments is intertwined with the recognition problem. \n",
    "\n",
    "In the next section, we will explore pixel neighborhood relations\n",
    "\n",
    "\n",
    "\n",
    "### Segmentation: Caveats\n",
    "\n",
    "Markov Random Fields (MRF) are a class of graphical models that are used to model the conditional probability of a random variable given its parents.\n",
    "\n",
    "MRF Nodes as Pixels\n",
    "\n",
    "![](Segmentation/img/mrf.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Energy Formulation\n",
    "Energy Function\\\n",
    "$E(x,y) = \\sum_{i} + \\sum_{i,j}\\psi (x_i,x_j)$ \\\n",
    "$ -logp(x,y) = - \\sum_{i} \\log \\psi(x_i,y_i)$\n",
    "\n",
    "Local Optima of Energy Function\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
