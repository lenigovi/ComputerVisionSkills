{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "\n",
    "The goal of segmentation is identify the boundaries between different objects in an image, and to simplify the representation of an image into meaningful boundaries that are easier to analyze. This topic is important because:\n",
    "- it is a more basic step for the convolutional filters in Neural Networks for extracting image features\n",
    "- it is the basis for many other image processing tasks such as object detection, object tracking, and image classification\n",
    "\n",
    "\\\n",
    "Image segmentation can be classified into three categories:\n",
    "Semantic segmentation\n",
    "Instance segmentation\n",
    "Panoptic segmentation\n",
    "\n",
    "The document will cover the following topics:\n",
    "- Segmentation as pixel wise classification\n",
    "    - Probabilistic classification\n",
    "    - Mixture of Gaussians, EM\n",
    "\n",
    "- Segmentation as energy minimization\n",
    "    - Markov Random Fields\n",
    "    - Energy formulation\n",
    "\n",
    "- Graph cuts for image segmentaton\n",
    "    - Basic idea\n",
    "    - s-t Mincut Algorithm\n",
    "    - Extension to non-binary case\n",
    "\n",
    "\n",
    "\n",
    "\\\n",
    "Definition of The Problem\\\n",
    "Identifying groups of pixels in the input which is the image. This is a semantic segmentation, this means all the pixels that belong to a title are grouped together. For example, semantically meaningful groups such as color similarity,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Segmentation Approaches:\n",
    "- Unsupervised Clustering\n",
    "    - Grouping what \"looks similar\"\n",
    "- Semantic Segmentation\n",
    "    - Learn a classifier to assign a semantic class $C_k$ to every pixel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation as Pixel-Wise Classification\n",
    "\n",
    "To define the grouping semantics, we use feature space. Grayscale image pixels are classified based on intensity similarity as a 1D representation, while colored images are classified based on color value similarity as a 3D representation.\n",
    "\n",
    "A filter Bank of 24 filters allow us to operate in 24-dimension. \n",
    "\n",
    "\n",
    "\\\n",
    "Basically the method is to apply a threshold, above which is the foreground, and below is the background.\n",
    "\n",
    "\\\n",
    "Probabilistic Classification\n",
    "- Bayesian Classification\n",
    "    - Given a measurement $x$, what semantic class $C_k$ should we assign to a pixel?\n",
    "    - We must recall Bayes Decision Theory in this section:\n",
    "        - $P(C_k|x) = \\frac{p(x|C_k)p(C_k)}{\\sum_{j}p(x|C_j)p(C_j)} $\n",
    "        - where: \n",
    "            - $p(x|C_k)$ is the likelihood of the measurement $x$ having been generated by class $C_k$\n",
    "            -  $p(C_k)$ is the prior probability of class $C_k$\n",
    "    - In order to build a classifier, we can try either:\n",
    "        - Discriminative Methods: directly estimating the posterior\n",
    "        - Generative Methods: estimating likelihood and prior, and then using the Bayes Decision formula\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In image recognition, a machine learning model can be taught to recognize objects. Next section will explore how machines recognize classifications by finding patterns. In order to explore the segmentation subject further, we need to take a look at Machine Learning Data Models first to understand the context better.\n",
    "\n",
    "\n",
    "### Machine Learning Data Models\n",
    "\n",
    "A machine Learning data model is a program that expresses the relationship between data and finds patterns in the dataset. When an unseen dataset is expressed in a ML data model, the model creates meaningful connections between data, and this can help us make decisions. For example, in natural language processing, machine learning models can parse and correctly recognize the context behind previously unheard sentences or combinations of words.\n",
    "\n",
    "Machine learning data models can be classified into two types: Generative and Discriminative Models\n",
    "\n",
    "- A generative model focuses on learning the underlying probability distribution of a given dataset. The fundamental idea behind generative models is to create a model that can generate new data points statistically similar to the original dataset, and new data points are generated from the probability distribution. This model type learns the patterns between data, and creates new realistic networks stemming from one value. This is why, the model focuses on learning the probability distribution that generates data, rather than the classification of data.\n",
    "Examples:\n",
    "    - Image or Face generation with Generative adversarial networks (GANs) \n",
    "    - Text generation\n",
    "    - Anomaly detection\n",
    "    - Gaussian mixture model learning the parameters of the Gaussian mixture that best fits the data \\\n",
    "&nbsp;\n",
    "\n",
    "- A discriminative model focuses on learning which x-value will map to which y-value. In other words, it learns the direct mapping between input variables and output labels (aka the classification of data) through learned boundaries without considering the underlying probability distribution of the data. The discriminative model learns to find the decision boundary that separates different classes or categories in the input space. This model type can make predictions on previously unseen data based on conditional probability and can be used either for classification or regression problem statements. \n",
    "For example: \n",
    "    - A convolutional neural network recognizing what an object in an image is\n",
    "    - A program that predicts the price of a house based on its features\n",
    "    - Logistic regression program performing sentiment analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "  <img src=\"Segmentation\\img\\generativedatamodel.jpg\" width=45% />\n",
    "  <img src=\"Segmentation\\img\\discriminativedatamodel.jpg\" width=45% /> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex;\">\n",
    "        <div style =\"padding-left\" 10>\n",
    "            <h3>Generative Model</h3>\n",
    "            <p>A generative model focuses on learning the underlying probability distribution of a given dataset. The fundamental idea behind generative models is to create a model that can generate new data points statistically similar to the original dataset, and new data points are generated from the probability distribution. This model type learns the patterns between data, and creates new realistic networks stemming from one value. This is why, the model focuses on learning the probability distribution that generates data, rather than the classification of data.\n",
    "            \n",
    "Examples:\n",
    "<ul>\n",
    "<li>Image or Face generation with Generative adversarial networks (GANs)</li>\n",
    "<li>Text generation</li>\n",
    "<li>Anomaly detection</li>\n",
    "<li>Gaussian mixture model learning the parameters of the Gaussian mixture that best fits the data</li>\n",
    "</ul>\n",
    "</p>\n",
    "        </div>\n",
    "        <div style =\"padding-right\" 10>\n",
    "            <h3>Discriminative Model</h3>\n",
    "            <p>A discriminative model focuses on learning which x-value will map to which y-value. In other words, it learns the direct mapping between input variables and output labels (aka the classification of data) through learned boundaries without considering the underlying probability distribution of the data. The discriminative model learns to find the decision boundary that separates different classes or categories in the input space. This model type can make predictions on previously unseen data based on conditional probability and can be used either for classification or regression problem statements.\n",
    "            \n",
    "Examples:\n",
    "<ul>\n",
    "<li>A convolutional neural network recognizing what an object in an image is</li>\n",
    "<li>A program that predicts the price of a house based on its features</li>\n",
    "<li>Logistic regression program performing sentiment analysis</li>\n",
    "<ul>\n",
    "</p>\n",
    "        </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Mixture Model of Gaussian Distribution (MoG)\n",
    "A mixture model is a statistical model used for representing data that may arise from a mixture of different probability distributions. In simpler terms, it's a way to describe data that might come from several different sources or populations.\n",
    "\n",
    "For estimating the parameters of a mixture model, we determine component distributions and their corresponding weights, with methods like expectation-maximization (EM) algorithm or Bayesian inference.\n",
    "\n",
    "Mixture Model of Gaussian Distribution (MoG) is a specific type of mixture model where the data distributions are Gaussian (also known as normal) distributions. In a Gaussian mixture model, the assumption is that the observed data is generated by a mixture of several Gaussian distributions with different means and variances. Therefore it is a generative data model.\n",
    "\n",
    "Mixture Model of Gaussian Distribution will be referred as MoG here on. MoG can be expressed so:\n",
    "$$ p(x) = \\sum_{i=1}^{K} \\phi_i \\mathcal{N}(x|\\mu_i, \\Sigma_i) $$\n",
    "\n",
    "where,\n",
    "- $p(x)$ is the probability density function of the mixture model\n",
    "- $\\phi_i$ is the mixing coefficient for the $i$-th component\n",
    "- $\\mathcal{N}(x|\\mu_i, \\Sigma_i)$ represents the Gaussian distribution with mean $\\mu_i$ and covariance matrix $\\Sigma_i$\n",
    "- $K$ is the number of components in the mixture\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation-Maximization (EM) Algorithm\n",
    "THe Expectation-maximization algorithm (EM), is a widely-used computational method for performing maximum likelihood method in certain models.\n",
    "\n",
    " - E-Step: assign samples to mixture model components:\n",
    "    - $ \\pi_j \\gamma_j(x_n) $\n",
    "\n",
    "\n",
    "User Assisted Image Segmentation\n",
    "- User marks two regions for foreground and background\n",
    "- Learn a MoG model for the color values in each region\n",
    "- Use the models to classify all pixels by deciding for the class with the highest posterior probability\n",
    "\n",
    "### Pros of MoG, EM\n",
    "- It provides an interpretation of the task probability functions\n",
    "- It is a generative model as the values can be generated from the distribution, and it can predict novel data points\n",
    "\n",
    "#### Cons of MoG, EM\n",
    "- Local minima\n",
    "    - k-means is NP-hard (see: computational complexity theory, nondeterministic polynomial time) even with k=2\n",
    "- Initialization\n",
    "    - Often a good idea to start with some k-means iteration\n",
    "- Needs to know number of componens\n",
    "    - Solution: Model selection\n",
    "- Needs careful implementation to avoid numerical issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveats\n",
    "\n",
    "So far we have explored  bottom-up ways for segmentation, for which we examined individual pixels and neighborhoods to segment an image into regions. Due to the problem for recognition in finding meaningful segments, alternative methods are explored. \n",
    "\n",
    "In the next section, we will explore pixel neighborhood relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References for this Section\n",
    "\n",
    "---\n",
    "\n",
    "[1] v7labs, https://www.v7labs.com/blog/panoptic-segmentation-guide\n",
    "\n",
    "[2] fiveMinuteStats, EM, https://stephens999.github.io/fiveMinuteStats/intro_to_em.html\n",
    "\n",
    "[3] fiveMinuteStates, Mixture Models, https://stephens999.github.io/fiveMinuteStats/intro_to_mixture_models.html\n",
    "\n",
    "[4] neptune.ai, https://neptune.ai/blog/image-segmentation\n",
    "\n",
    "[5] Mordatch, Igor, \"Concept Learning with Energy-Based Models\" OpenAI. https://openreview.net/pdf?id=H12Y1dJDG\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation as Energy Minimization\n",
    "\n",
    "In the previous section, we explored semantioc clustering approaches for segmentation. In this section, we will explore Image Segmentation with Energy Minimization methods.\n",
    "\n",
    "### Markov Random Fields Method\n",
    "\n",
    "Markov Random Fields (MRF) is a method to model a joint distribution of an undirected, connected graph where each node implies a random variable and each edge between nodes is a modeled stochastic dependency.\n",
    "\n",
    "MRF is an undirected graphical model that explicitly expresses the conditional independence relationship between nodes.\n",
    "\n",
    "In MRF, we use a class of graphical models to model the conditional probability of a random variable with its given parents.\n",
    "\n",
    "MRF Nodes as Pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Segmentation/img/mrf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Energy Formulation\n",
    "Energy Function\\\n",
    "$E(x,y) = \\sum_{i} + \\sum_{i,j}\\psi (x_i,x_j)$ \\\n",
    "$ -logp(x,y) = - \\sum_{i} \\log \\psi(x_i,y_i)$\n",
    "\n",
    "Local Optima of Energy Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References for this Section\n",
    "\n",
    "[1] Jun-DevpBlog https://medium.com/jun94-devpblog/cv-7-segmentation-as-energy-minimization-markov-random-fields-energy-formulation-graph-cut-670b9b3c82ee\n",
    "\n",
    "[2] Statistical Techniques in Robotics, CMU https://www.cs.cmu.edu/~16831-f14/notes/F11/16831_lecture07_bneuman.pdf\n",
    "\n",
    "[3] https://jwmi.github.io/ASM/Murphy%20chapter%2019.pdf"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
