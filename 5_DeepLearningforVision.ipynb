{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Vision\n",
    "\n",
    "- Convolution Operation\n",
    "- Pooling\n",
    "- Convolutional Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "## Convolution Operation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "\n",
    "Neural Networks vs Convolutional Neural Networks\n",
    "In a typical neural network, each neuron in the input layer is connected to a neuron in the hidden layer. However, in a CNN, only a small region of input layer neurons connect to neurons in the hidden layer. These regions are referred to as local receptive fields, which is translated across an image to create a feature map from the input layer to the hidden layer neurons. To implement this process efficiently, convolution is applied.\n",
    "\n",
    "In CNNs, the weights and bias values are the same for all hidden neurons in a given layer. This means that all hidden neurons are detecting the same feature such as an edge or a blob in different regions of the image. This makes the network tolerant to translation of objects in an image. For example, a network trained to recognize cats, will be able to do so, whenever the cat is in the image.\n",
    "\n",
    "### Activation and Pooling\n",
    "The activation step applies a transformation the the output of the neuron by using activation functions. Rectified Linear Unit, or ReLU, is a commonly used activation function. It takes the output of a neuron and maps it to the highest positive value. If the output is negative, the function maps it to zero.\n",
    "\n",
    "Pooling further transforms the output of the activation step and reduces the dimensionality of the featured map by condensing the output of small regions of neurons into a single output, which helps simplifying the following layers, and reduces the number of parameters that the model needs to learn.\n",
    "\n",
    "Every hidden layer increases the complexity of the learned image features. For example, the first layer might learn how to detect edges, and the last layer might learn how to detect more complex shapes.\n",
    "\n",
    "### Backpropagation\n",
    "Short for \"backward propagation of errors\", it is a supervised algorithm used to minimize errors in predictions made by neural networks. The chain rule in calculus is applied: \n",
    "$$\\frac{dx}{du}=\\frac{du}{dy}\\times \\frac{dx}{dv}$$\n",
    "\n",
    "If $y=g(u)$ and $u=f(x)$ then $y=g(f(x))$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
